{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1\n",
    "Explain One-Hot Encoding.\n",
    "\n",
    "Answer:<br>\n",
    "One hot encoding can be defined as the essential process of converting the categorical data variables to be provided to machine and deep learning algorithms which in turn improve predictions as well as classification accuracy of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2\n",
    "Explain Bag of Words\n",
    "\n",
    "Answer:<br>\n",
    "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\n",
    "\n",
    "The approach is very simple and flexible, and can be used in a myriad of ways for extracting features from documents.\n",
    "\n",
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "1. A vocabulary of known words.\n",
    "2. A measure of the presence of known words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3\n",
    "Explain Bag of N-Grams\n",
    "\n",
    "Answer:<br>\n",
    "A bag-of-n -grams model is a way to represent a document, similar to a [bag-of-words][/terms/bag-of-words/] model. A bag-of-n -grams model represents a text document as an unordered collection of its n -grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4\n",
    "Explain TF-IDF\n",
    "\n",
    "Answer:<br>\n",
    "The TF*IDF algorithm is used to weigh a keyword in any content and assign importance to that keyword based on the number of times it appears in the document. More importantly, it checks how relevant the keyword is throughout the web, which is referred to as corpus.\n",
    "\n",
    "For a term t in document d, the weight Wt,d of term t in document d is given by:\n",
    "\n",
    "Wt,d = TFt,d log (N/DFt)\n",
    "\n",
    "Where:\n",
    "\n",
    "- TFt,d is the number of occurrences of t in document d.\n",
    "- DFt is the number of documents containing the term t.\n",
    "- N is the total number of documents in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 5\n",
    "What is OOV problem?\n",
    "\n",
    "Answer:<br>\n",
    "An OOV value is associated with values not seen by the model at training time. Hence, if we get an OOV value at inference time, the model won't know what to do with it. One simple solution is to replace all the rare values with a special OOV token before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 6\n",
    "What are word embeddings?\n",
    "\n",
    "Answer:<br>\n",
    "In natural language processing, word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 7\n",
    "Explain Continuous bag of words (CBOW).\n",
    "\n",
    "Answer:<br>\n",
    "This method takes the context of each word as the input and tries to predict the word corresponding to the context. The input or the context word is a one hot encoded vector of size V. The hidden layer contains N neurons and the output is again a V length vector with the elements being the softmax values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 8\n",
    "Explain SkipGram.\n",
    "\n",
    "Answer:<br>\n",
    "The main idea behind the Skip-Gram model is this: it takes every word in a large corpora (we will call it the focus word) and also takes one-by-one the words that surround it within a defined ‘window’ to then feed a neural network that after training will predict the probability for each word to actually appear in the window around the focus word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 9\n",
    "Explain Glove Embeddings.\n",
    "\n",
    "Answer:<br>\n",
    "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. The resulting embeddings show interesting linear substructures of the word in vector space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
